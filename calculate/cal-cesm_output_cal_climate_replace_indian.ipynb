{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8358c21c-7789-49f0-9d8a-0ebf522a66cb",
   "metadata": {},
   "source": [
    "# 2022/4/6\n",
    "# 本代码计算CESM输出文件的climate\n",
    "# 实验是把印度大陆替换为海洋的实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92160e98-d858-4d92-b4b9-3bb01b79cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "module_path = [\"/home/sun/mycode/module/\",\"/data5/2019swh/mycode/module/\"]\n",
    "sys.path.append(module_path[0])\n",
    "from module_sun import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc47f9a-1f58-4cd2-bf67-f4bf0b043bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in  =  \"/home/sun/cesm_output/replace_india8/atm/vinth2p/\"\n",
    "path_out =  \"/home/sun/data/cesm_output/\"\n",
    "\n",
    "year_range  =  np.array([1981,1998],dtype=int)\n",
    "\n",
    "f0  =  xr.open_dataset(path_in+'replace_india8.cam.h1.1993-03-13-00000.nc')\n",
    "vars  =  [\"LHFLX\",\"OMEGA\",\"PRECT\",\"Q\",\"SHFLX\",\"T\",\"U\",\"V\",\"Z3\",\"PS\",\"TS\"]\n",
    "\n",
    "# 定义原始的climate变量\n",
    "\n",
    "#三维\n",
    "lhflx     =  np.zeros((365,192,288))\n",
    "prect     =  lhflx.copy()\n",
    "shflx     =  lhflx.copy()\n",
    "ps        =  lhflx.copy()\n",
    "ts        =  lhflx.copy()\n",
    "\n",
    "#四维\n",
    "omega     =  np.zeros((365,37,192,288))\n",
    "q         =  omega.copy()\n",
    "t         =  omega.copy()\n",
    "u         =  omega.copy()\n",
    "v         =  omega.copy()\n",
    "z3        =  omega.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf70cc-3c0b-4bee-ae6b-e229bee436be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for yyyy in range(year_range[0],year_range[1]+1):\n",
    "    # 先获取这一年的文件列表\n",
    "    list0  =  []\n",
    "    for f1 in os.listdir(path_in):\n",
    "        if \".h1.\"+str(yyyy) in f1:\n",
    "            list0.append(f1)\n",
    "    list0.sort()\n",
    "    \n",
    "    j = 0 # j 0->365 account\n",
    "    for f2 in list0:\n",
    "        f3  =  xr.open_dataset(path_in+f2)\n",
    "        lhflx[j,:]  +=  f3.LHFLX.data[0,:]\n",
    "        prect[j,:]  +=  f3.PRECT.data[0,:]\n",
    "        shflx[j,:]  +=  f3.SHFLX.data[0,:]\n",
    "        ps[j,:]     +=  f3.PS.data[0,:]\n",
    "        ts[j,:]     +=  f3.TS.data[0,:]\n",
    "\n",
    "        omega[j,:]  +=  f3.OMEGA.data[0,:]\n",
    "        q[j,:]      +=  f3.Q.data[0,:]\n",
    "        t[j,:]      +=  f3.T.data[0,:]\n",
    "        u[j,:]      +=  f3.U.data[0,:]\n",
    "        v[j,:]      +=  f3.V.data[0,:]\n",
    "        z3[j,:]      +=  f3.Z3.data[0,:]\n",
    "\n",
    "        j +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f11ed74f-8582-4982-b30e-96e920e2145c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-155.81819216668123"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成文件\n",
    "pnew     = np.array([1000, 975, 950, 925, 900, 875, 850, 825, 800, 775, 750, 700, 650, 600, 550, 500, 450, 400, 350, 300, 250, 225, 200, 175, 150, 125, 100, 70, 50, 30, 20, 10, 7, 5, 3, 2, 1])\n",
    "ds    = xr.Dataset(\n",
    "    {\n",
    "        \"LHFLX\":([\"time\",\"lat\",\"lon\"],lhflx),\n",
    "        \"OMEGA\":([\"time\",\"lev\",\"lat\",\"lon\"],omega),\n",
    "        \"PRECT\":([\"time\",\"lat\",\"lon\"],prect),\n",
    "        \"Q\":([\"time\",\"lev\",\"lat\",\"lon\"],q),\n",
    "        \"SHFLX\":([\"time\",\"lat\",\"lon\"],shflx),\n",
    "        \"T\":([\"time\",\"lev\",\"lat\",\"lon\"],t),\n",
    "        \"U\":([\"time\",\"lev\",\"lat\",\"lon\"],u),\n",
    "        \"V\":([\"time\",\"lev\",\"lat\",\"lon\"],v),\n",
    "        \"Z3\":([\"time\",\"lev\",\"lat\",\"lon\"],z3),\n",
    "        \"PS\":([\"time\",\"lat\",\"lon\"],ps),\n",
    "        \"TS\":([\"time\",\"lat\",\"lon\"],ts),\n",
    "    },\n",
    "    coords={\n",
    "        \"lon\":([\"lon\"],f0.lon.data),\n",
    "        \"lat\":([\"lat\"],f0.lat.data),\n",
    "        \"time\":([\"time\"],np.linspace(1,365,365)),\n",
    "        \"lev\":([\"lev\"],pnew)\n",
    "    },\n",
    ")\n",
    "ds.LHFLX.attrs     =      f0.LHFLX.attrs\n",
    "ds.OMEGA.attrs     =      f0.OMEGA.attrs\n",
    "ds.PRECT.attrs     =      f0.PRECT.attrs\n",
    "ds.Q.attrs         =      f0.Q.attrs\n",
    "ds.SHFLX.attrs     =      f0.SHFLX.attrs\n",
    "ds.T.attrs         =      f0.T.attrs\n",
    "ds.U.attrs         =      f0.U.attrs\n",
    "ds.V.attrs         =      f0.V.attrs\n",
    "ds.Z3.attrs        =      f0.Z3.attrs\n",
    "ds.PS.attrs        =      f0.PS.attrs\n",
    "ds.TS.attrs        =      f0.TS.attrs\n",
    "ds.lon.attrs       =      f0.lon.attrs\n",
    "ds.lat.attrs       =      f0.lat.attrs\n",
    "ds.time.attrs      =      f0.time.attrs\n",
    "ds.lev.attrs[\"units\"]    =      \"hPa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20feba9-7fb4-4232-bbde-ccd1aff1e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf(path_out+\"replace_indian.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
